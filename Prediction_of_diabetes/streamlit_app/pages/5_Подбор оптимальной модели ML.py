import streamlit as st
from set_params import load_page_setting

st.set_page_config(
    page_title="ML search",
    page_icon="üìü"
)

# –∑–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ñ–æ–Ω–∞ —Å—Ç–∏–ª–µ–π –∏ —Ç.–¥.
load_page_setting()

st.sidebar.header("ML search")
st.sidebar.success("–¢—é–Ω–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π –∏ –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –ø–æ –º–µ—Ç—Ä–∏–∫–µ")


st.markdown("""
# –ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ ML

–ù–∞—á–Ω–µ–º —Å LogisticRegression. –°–¥–µ–ª–∞–µ–º 2 –æ–±—É—á–µ–Ω–∏—è - –±–µ–∑ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ —Å –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
""")
with st.expander("LogisticRegression"):
    st.code("""
    model_logreg = LogisticRegression(random_state=RANDOM_STATE)

    params_logreg = {
        'penalty': ['l1', 'l2', 'elasticnet', None],
        'class_weight' : ['balanced', None],
        'solver': ['liblinear', 'sag', 'saga'],
        'max_iter': [x for x in range(20, 201, 20)],
        'l1_ratio': [0, 0.5, 1, None]
    }

    cv = ShuffleSplit(n_splits=3, test_size=0.10, random_state=RANDOM_STATE)

    gs_logreg = GridSearchCV(
        model_logreg,
        params_logreg,
        n_jobs=-1,
        cv=cv,
        scoring="f1_macro",
        verbose = 1,
        return_train_score = True

    )
            
    %%time
    # –¥–∞–Ω–Ω—ã–µ –Ω–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã
    gs_logreg.fit(x_train, y_train)
            
    predict = gs_logreg.best_estimator_.predict(x_val)
    print(f'f1 score LogisticRegression = {f1_score(y_val, predict, average = "macro")}')
    print(f'–ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {gs_logreg.best_params_}')
    print(f'–ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = {gs_logreg.best_score_}')
            
    f1 score LogisticRegression = 0.5733333333333333
    –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {'class_weight': 'balanced', 'l1_ratio': 0, 'max_iter': 20, 'penalty': 'l2', 'solver': 'liblinear'}
    –ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = 0.6324795693082138

    %%time
    # –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã
    gs_logreg.fit(pipeline_transform.transform(x_train), y_train)

    predict = gs_logreg.best_estimator_.predict(pipeline_transform.transform(x_val))
    print(f'f1 score LogisticRegression –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–ª–∏—Ä–æ–≤–∞–Ω—ã= {f1_score(y_val, predict, average = "macro")}')
    print(f'–ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {gs_logreg.best_params_}')
    print(f'–ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = {gs_logreg.best_score_}')
            
    f1 score LogisticRegression –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–ª–∏—Ä–æ–≤–∞–Ω—ã= 0.5465587044534412
    –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {'class_weight': None, 'l1_ratio': 0, 'max_iter': 20, 'penalty': 'l1', 'solver': 'saga'}
    –ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = 0.636005530417295


    """, line_numbers = True)

st.markdown("""
    –¢–µ–ø–µ—Ä—å –≤–æ–∑—å–º–µ–º KNeighborsClassifier
""")

with st.expander("KNeighborsClassifier"):
    st.code("""
    model_knn =  KNeighborsClassifier()

    params_knn = {
        'n_neighbors': [x for x in range(5, 46, 5)],
        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
        'p': [1, 2]
    }

    cv = ShuffleSplit(n_splits=3, test_size=0.10, random_state=RANDOM_STATE)

    gs_knn = GridSearchCV(
        model_knn,
        params_knn,
        n_jobs=-1,
        cv=cv,
        scoring="f1_macro",
        verbose = 1,
        return_train_score = True
    )
    %%time
    # –¥–∞–Ω–Ω—ã–µ –Ω–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã
    gs_knn.fit(x_train, y_train)  

    predict = gs_knn.best_estimator_.predict(x_val)
    print(f'f1 score KNeighborsClassifier = {f1_score(y_val, predict, average = "macro")}')
    print(f'–ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {gs_knn.best_params_}')
    print(f'–ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = {gs_knn.best_score_}')

    f1 score KNeighborsClassifier = 0.3552492046659597
    –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {'algorithm': 'auto', 'n_neighbors': 25, 'p': 1}
    –ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = 0.5369221578898998
            
    %%time
    # –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã

    gs_knn.fit(pipeline_transform.transform(x_train), y_train)

            
    predict = gs_knn.best_estimator_.predict(pipeline_transform.transform(x_val))
    print(f'f1 score KNeighborsClassifier –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã = {f1_score(y_val, predict, average = "macro")}')
    print(f'–ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {gs_knn.best_params_}')
    print(f'–ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = {gs_knn.best_score_}')
            
    f1 score KNeighborsClassifier –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã = 0.7408906882591093
    –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {'algorithm': 'auto', 'n_neighbors': 15, 'p': 1}
    –ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = 0.6650736758263639
""",
line_numbers = True)
    
st.markdown("""
    –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –±—É—Å—Ç–∏–Ω–≥–∞–º. –î–ª—è –Ω–∞—á–∞–ª–∞ LGBMClassifie
""")

with st.expander("LGBMClassifier"):
    st.code("""
    model_lgbm = LGBMClassifier(random_state=RANDOM_STATE)

params_lgbm = {
    'n_estimators': [x for x in range(40, 80, 20)],
    'learning_rate': [0.1, 0.01],
}

cv = ShuffleSplit(n_splits=3, test_size=0.10, random_state=RANDOM_STATE)

gs_lgbm = GridSearchCV(
    model_lgbm,
    params_lgbm,
    n_jobs=-1,
    cv=cv,
    scoring="f1_macro",
    verbose = 0,
    return_train_score = True
)
            
    %%time
    # –¥–∞–Ω–Ω—ã–µ –Ω–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã
    gs_lgbm.fit(x_train.values, y_train.values)
                

    predict = gs_lgbm.best_estimator_.predict(x_val)
    print(f'f1 score LGBMClassifier = {f1_score(y_val, predict, average = "macro")}')
    print(f'–ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {gs_lgbm.best_params_}')
    print(f'–ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = {gs_lgbm.best_score_}')
                

    f1 score LGBMClassifier = 0.716256157635468
    –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {'learning_rate': 0.1, 'n_estimators': 60}
    –ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = 0.722039072039072
            

    %%time
    # –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã
    gs_lgbm.fit(pipeline_transform.transform(x_train), y_train.values)

    predict = gs_lgbm.best_estimator_.predict(pipeline_transform.transform(x_val))
    print(f'f1 score LGBMClassifier –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã = {f1_score(y_val, predict, average = "macro")}')
    print(f'–ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {gs_lgbm.best_params_}')
    print(f'–ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = {gs_lgbm.best_score_}')
                
    f1 score LGBMClassifier –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã = 0.6761133603238867
    –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {'learning_rate': 0.1, 'n_estimators': 40}
    –ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–¥–∏–∞—Ü–∏–∏ = 0.709807055850808

""", line_numbers = True)
    
st.markdown("""
    –°–¥–µ–ª–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç –¥–ª—è catboost, –¥–∞—Å—Ç –ª–∏ —ç—Ñ—Ñ–µ–∫—Ç –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
""")

with st.expander("–õ–æ–∫–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç 1"):
    st.code("""
%%time
# —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–∞–∑–∞–æ–≤–æ–π –º–æ–¥–µ–ª–∏ catboost

# —Å–¥–µ–ª–∞–µ–º –∫–æ–ø–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞
data_base = data_eda_1v1_drop.copy()

x_train_all = data_base.drop('–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç (–ï–°–¢–¨\–ù–ï–¢)', axis = 1)
y_train_all = data_base['–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç (–ï–°–¢–¨\–ù–ï–¢)']

x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, test_size = 0.10, shuffle = True, random_state = RANDOM_STATE)

model = CatBoostClassifier(random_state = RANDOM_STATE, verbose = 0)
model.fit(x_train, y_train)
predict = model.predict(x_val)
print(f'f1 score —É baseline –º–æ–¥–µ–ª–∏ (pipeline encoder)= {f1_score(y_val, predict, average = "macro")}')
f1 score —É baseline –º–æ–¥–µ–ª–∏ (pipeline encoder)= 0.7117117117117118
""",
line_numbers = True)


with st.expander("–õ–æ–∫–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç 2"):
    st.code("""
%%time
# —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–∞–∑–∞–æ–≤–æ–π –º–æ–¥–µ–ª–∏ catboost - pipeline encoder

x_all = data_base.drop('–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç (–ï–°–¢–¨\–ù–ï–¢)', axis = 1)
y_all = data_base['–°–∞—Ö–∞—Ä–Ω—ã–π –¥–∏–∞–±–µ—Ç (–ï–°–¢–¨\–ù–ï–¢)']

x_train, x_val, y_train, y_val = train_test_split(x_all, y_all, test_size = 0.10, shuffle = True, random_state = RANDOM_STATE)

model = CatBoostClassifier(random_state = RANDOM_STATE, verbose = 0)
model.fit(pipeline_transform.transform(x_train), y_train)
predict = model.predict(pipeline_transform.transform(x_val))
print(f'f1 score —É baseline –º–æ–¥–µ–ª–∏ (pipeline encoder)= {f1_score(y_val, predict, average = "macro")}')
f1 score —É baseline –º–æ–¥–µ–ª–∏ (pipeline encoder)= 0.6389743589743591
""", line_numbers = True)


st.markdown("""
–≠–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ catboost –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å –±–µ–∑ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            
–î–ª—è —Ç—é–Ω–∏–Ω–≥–∞ CatBoostClassifier –∏ RandomForestClassifier, –∏ –ø–æ—Å–∏–∫–∞ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –±–∏–±–∏–æ–∏–æ—Ç–µ–∫–æ–π optuna
""")

with st.expander("–ü–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è CatBoostClassifier"):
    st.code("""# –ø–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è CatBoostClassifier

    def CatBoostClassifier_p1(trial):
        '''—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è optuna'''

        # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        iterations = trial.suggest_int("iterations", 5, 500)
        depth = trial.suggest_int("depth", 1, 16)
        learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-1)
        l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 2, 3.5)

        print('–í—ã–±—Ä–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: ', iterations, depth, learning_rate, l2_leaf_reg)

        pipeline = Pipeline(steps=[
        ('CatBoost', CatBoostClassifier(iterations = iterations,
                            depth = depth,
                            learning_rate = learning_rate,
                            l2_leaf_reg = l2_leaf_reg,
                            random_state = RANDOM_STATE,
                            verbose = 0))
        ])


        cv = ShuffleSplit(n_splits=3, test_size=0.10, random_state=RANDOM_STATE)

        start = time.time()

        scores = cross_val_score(pipeline, x_train_all, y_train_all, scoring = 'f1_macro',  cv=cv, verbose = 1)

        end = time.time()

        print(f'–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–∞–≤–∏–ª–æ = {round(end - start, 2)} —Å–µ–∫—É–Ω–¥')
        print()

        return scores.mean()
     
    %%time
    study_CB = optuna.create_study(direction="maximize")
    study_CB.optimize(CatBoostClassifier_p1, n_trials=20)
    study_CB.best_params
            
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è CatBoost
    {'iterations': 437,
    'depth': 2,
    'learning_rate': 0.059429953553017466,
    'l2_leaf_reg': 2.0210035460360274}   Best is trial 8 with value: 0.7208769444063563.""", line_numbers = True)

with st.expander("–ü–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è RandomForestClassifier"):

    st.code("""# –ø–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è RandomForestClassifier

    def objective_RF(trial):
        '''—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è optuna'''
        # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        n_estimators = trial.suggest_int("n_estimators", 10, 180)
        criterion = trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss'])
        max_depth = trial.suggest_int("max_depth", 8, 25)
        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])
        min_samples_split = trial.suggest_categorical('min_samples_split', [2, 3, 4])
        min_samples_leaf = trial.suggest_categorical('min_samples_leaf', [1, 2, 3])
        class_weight = trial.suggest_categorical("class_weight", ["balanced", None])


        pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('RandomForestClassifier', RandomForestClassifier(n_estimators  = n_estimators ,
                                criterion = criterion ,
                                    max_depth= max_depth,
                                    max_features = max_features,
                                    min_samples_split = min_samples_split,
                                    min_samples_leaf = min_samples_leaf,
                                    class_weight = class_weight,
                                    random_state = RANDOM_STATE, n_jobs = -1))
        ])


        # —á–µ—Ä–µ–∑ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é —Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        cv = ShuffleSplit(n_splits=3, test_size=0.1, random_state=RANDOM_STATE)

        start = time.time()

        cores = cross_val_score(pipeline, x_train_all, y_train_all, scoring = 'f1_macro',  cv=cv, verbose = 1)

        end = time.time()

        print(f'–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–∞–≤–∏–ª–æ = {round(end - start, 2)} —Å–µ–∫—É–Ω–¥')
        print()

        return scores.mean()

    # %%time
    study_RF_p = optuna.create_study(direction="maximize")
    study_RF_p.optimize(objective_RF, n_trials=100)
    study_RF_p.best_params

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    {'n_estimators': 88,
    'criterion': 'gini',
    'max_depth': 14,
    'max_features': 'log2',
    'min_samples_split': 3,
    'min_samples_leaf': 2,
    'class_weight': None} Best is trial 61 with value: 0.7156821672950705.""", line_numbers = True)

st.markdown(
"""
### –í—ã–≤–æ–¥—ã
–ü—É—Ç–µ–º –ø–µ—Ä–µ–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ, –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é –æ–∫–∞–∑–∞–ª—Å—è Catboost c –º–µ—Ç—Ä–∏–∫–æ–π f1 macro 0.72.
"""
)